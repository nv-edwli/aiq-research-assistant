general:
  use_uvloop: true
  front_end:
    _type: fastapi
    endpoints:
      - path: /generate_query
        method: POST
        description: Creates the query
        function_name: generate_query
      - path: /generate_summary
        method: POST
        description: Generates the summary
        function_name: generate_summary
      - path: /artifact_qa
        method: POST
        description: Q/A or chat about a previously generated artifact
        function_name: artifact_qa
      - path: /aiqhealth
        method: GET
        description: Health check for the AIQ AIRA service
        function_name: health_check
      - path: /default_collections
        method: GET
        description: Get the default collections
        function_name: default_collections

llms:
  # The inst_llm is used for Q&A and report writing and should be an instruct model.
  # The default configuration below is assuming a docker compose deployment of AIRA 
  # that uses local NVIDIA NIM microservices.
  # Update if you are deploying differently or using hosted NVIDIA NIM microservices.
  instruct:
    _type: openai
    model_name: meta/llama-3.3-70b-instruct
    temperature: 0.0
    base_url: http://10.185.119.221:8050/v1
    api_key: not-needed
  # The reasoning llm is used for report planning and reflection and should be a reasoning model
  # that supports thinking tokens. The default configuration below is used assuming a docker compose
  # deployment of AIRA and RAG with a local NVIDIA NIM microservice for the nemotron model.
  # Update if you are deploying differently or using hosted NVIDIA NIM microservices.
  nemotron:
    _type: openai
    model_name : nvidia/llama-3.3-nemotron-super-49b-v1
    temperature: 0.5
    base_url: http://10.185.119.221:8999/v1 # default if deployed using RAG blueprint and docker compose
    max_tokens: 5000
    stream: true
    api_key: not-needed
  

functions:

  rag_search:
    _type: rag_search
    rag_url: http://rag-server:8081/v1

  tavily_search:
    _type: tavily_search
    max_results: 3

  generate_query:
    _type: generate_queries

  search_agent:
    _type: search_agent
    llm_name: instruct
    verbose: true
    tool_names:
      - rag_search
      - tavily_search

  generate_summary:
    _type: generate_summaries
    tool_names:
      - search_agent
      

  # artifact_qa:  
  artifact_qa:  
    _type: artifact_qa
    llm_name: instruct
    max_tool_calls: 3
    tool_names:
      - rag_search
      - tavily_search

  health_check:
    _type: health_check

  default_collections:
    _type: default_collections
    collections:
      - name: "Biomedical_Dataset"
        topic: "Biomedical"
        report_organization: "You are a medical researcher who specializes in cystic fibrosis. Create a report analyzing how CFTR modulators can be used to restore CFTR protein functions. Include a 150-200 word abstract and a methods, results, and discussion section. Format your answer in paragraphs. Consider all (and only) relevant data. Give a factual report with cited sources."
      - name: "Financial_Dataset"
        topic: "Financial"
        report_organization: "You are a financial analyst who specializes in financial statement analysis. Write a financial report analyzing the 2023 financial performance of Amazon. Identify trends in revenue growth, net income, and total assets. Discuss how these trends affected Amazon's yearly financial performance for 2023. Your output should be organized into a brief introduction, as many sections as necessary to create a comprehensive report, and a conclusion. Format your answer in paragraphs. Use factual sources such as Amazon's quarterly meeting releases for 2023. Cross analyze the sources to draw original and sound conclusions and explain your reasoning for arriving at conclusions. Do not make any false or unverifiable claims. I want a factual report with cited sources."


workflow:
  _type: ai_researcher
