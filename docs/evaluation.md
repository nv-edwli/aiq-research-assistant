# Evaluation 

The reports generated by the AI-Q Research Assistant have been internally evaluated with a custom evaluation framework. This framework assesses the quality of a generated report and report sources using adapted Ragas metrics. A ground truth report is used for comparison.

## Methodology 

Evaluation starts by generating a set of probing questions using the ground truth report. These questions are used to evaluate the context relevance metric of the AI-Q Research Assistant reports. Once the generated report, generated sources, and ground truth questions are available, the three metrics are calculated.


### Answer Accuracy Metric

The **AnswerAccuracy** metric in **Ragas** measures how closely a model's response matches a reference ground truth for a given question.

**How it works:**

- **Dual LLM Evaluation**: Uses two distinct "LLM-as-a-judge" prompts to evaluate the same response, providing robustness through multiple perspectives
- **Rating Scale**: Each LLM judge rates the response on a scale of **0, 2, or 4**:
  - **0**: The response is inaccurate or doesn't address the same question as the reference.
  - **2**: The response partially aligns with the reference.
  - **4**: The response exactly aligns with the reference.
- **Score Normalization**: The ratings are converted to a [0,1] scale by dividing by 4.
- **Final Score Calculation**: 
  - If both ratings are valid, the final score is the average of the two normalized scores.
  - If only one rating is valid, it takes the valid one.

---

### Context Relevance Metric

The **Context Relevance** metric evaluates whether the retrieved contexts (chunks or passages) are pertinent to the user's query.

**How it works:**

- **Dual LLM Evaluation**: Uses two distinct "LLM-as-a-judge" prompts to evaluate the relevance of retrieved contexts, providing robustness through multiple perspectives.
- **Rating Scale**: Each LLM judge rates relevance on a scale of **0, 1, or 2**:
  - **0**: The retrieved contexts are not relevant to the user's query at all.
  - **1**: The contexts are partially relevant.
  - **2**: The contexts are completely relevant.
- **Score Normalization**: The ratings are converted to a [0,1] scale by dividing by 2.
- **Final Score Calculation**:
  - If both ratings are valid, the final score is the average of the two normalized scores.
  - If only one rating is valid, it takes the valid one.

**Implementation Details:**

- The implementation uses two different prompt templates to ensure robustness:
  - **Template 1**: Instructs the LLM to evaluate the relevance of the context to the question.
  - **Template 2**: Provides a slightly different approach to the same evaluation task.
- Includes retry logic (default of 5 retries) if the rating is not found within the first 8 tokens of the LLM response.

---

### Groundedness Metric

The **Groundedness** metric evaluates how well a response is supported or "grounded" by the retrieved contexts. It assesses whether the claims in the response can be found in the provided contexts.

**How it works:**

- **Dual LLM Evaluation**: Similar to Answer Accuracy and Context Relevance metrics, it uses two distinct "LLM-as-a-judge" prompts to evaluate groundedness, providing robustness through multiple perspectives.
- **Rating Scale**: Each LLM judge rates groundedness on a scale of **0, 1, or 2**:
  - **0**: The response is not grounded in the context at all.
  - **1**: The response is partially grounded.
  - **2**: The response is fully grounded (every statement can be found or inferred from the retrieved context).
- **Score Normalization**: The ratings are converted to a [0,1] scale by dividing by 2.
- **Final Score Calculation**:
  - If both ratings are valid, the final score is the average of the two normalized scores.
  - If only one rating is valid, it takes the valid one.

**Implementation Details:**

- Uses two different prompt templates to ensure robustness.
- Includes retry logic (default of 5 retries) if the rating is not found within the first 8 tokens of the LLM response.
